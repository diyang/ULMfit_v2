{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "from nltk.corpus import stopwords\n",
    "import nbimporter\n",
    "import re\n",
    "from functions import prep\n",
    "import statistics\n",
    "import math\n",
    "from customLSTMcell import CustomCell\n",
    "from customLSTMcell import LayerNormalizedLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 18\n",
    "FORWARD_STEPS = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_epochs = 20\n",
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14640 tweets\n"
     ]
    }
   ],
   "source": [
    "# read data from twitter\n",
    "tweet_data   = pd.read_csv('datasets/us_airline.csv')\n",
    "columns = ['text', 'airline_sentiment']\n",
    "tweet_data_extract = tweet_data[:][columns]\n",
    "tweet_data_extract.dropna()\n",
    "tweet_data_extract[\"clean_text\"] = tweet_data_extract[\"text\"].map(lambda x: prep.text_to_wordlist(x))\n",
    "tweet_data_extract[\"labels\"] = tweet_data_extract[\"airline_sentiment\"].map(lambda x: prep.sentiment_to_label(x, ['neutral', 'positive', 'negative']))\n",
    "print('Found %s tweets' % len(tweet_data_extract[\"clean_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15074 tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenize tweets\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(tweet_data_extract[\"clean_text\"])\n",
    "sequence_tweets = tokenizer.texts_to_sequences(tweet_data_extract[\"clean_text\"])\n",
    "sequence_tweets_pad = pad_sequences(sequence_tweets, MAX_SEQUENCE_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe_file = \"datasets/glove.6B/glove.6B.300d.txt\"\n",
    "word2vec_file = \"datasets/glove_word2vec.txt\"\n",
    "embedding_matrix = prep.word2vec_GloVe(GloVe_file, word2vec_file, word_index=word_index)\n",
    "nb_words = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of train data: int32\n",
      "dtype of train label: int32\n",
      "shape of train data:  (13176, 18)\n",
      "shape of train label:  (13176,)\n"
     ]
    }
   ],
   "source": [
    "train_seq = sequence_tweets_pad\n",
    "label_seq = tweet_data_extract[\"labels\"]\n",
    "np_train_seq = np.array(list(train_seq), dtype='int32')\n",
    "np_label_seq = np.array(list(label_seq), dtype='int32')\n",
    "\n",
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(np_train_seq))\n",
    "idx_train = perm[:int(len(np_train_seq)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(np_train_seq)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train = np_train_seq[idx_train]\n",
    "data_val = np_train_seq[idx_val]\n",
    "\n",
    "labels_train = np_label_seq[idx_train]\n",
    "labels_val = np_label_seq[idx_val]\n",
    "\n",
    "print('dtype of train data: %s' % data_train.dtype)\n",
    "print('dtype of train label: %s' % labels_train.dtype)\n",
    "\n",
    "print('shape of train data: ', data_train.shape)\n",
    "print('shape of train label: ', labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAN STEP 412\n",
      "VALID STEP 46\n"
     ]
    }
   ],
   "source": [
    "steps_per_train_epoch = int(data_train.shape[0]/BATCH_SIZE)+1\n",
    "steps_per_valid_epoch = int(data_val.shape[0]/BATCH_SIZE)+1\n",
    "print('TRIAN STEP %d' % steps_per_train_epoch)\n",
    "print('VALID STEP %d' % steps_per_valid_epoch)\n",
    "\n",
    "data = {'train': data_train, 'labels': labels_train, 'embedding': embedding_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def train_network(g, data, num_epochs, batch_size = 32, verbose = True, save=False, pretrain_model=None):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if pretrain_model is not None:\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_model))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                print(\"Loading Model...\")\n",
    "                g[\"saver\"].restore(sess, ckpt.model_checkpoint_path)\n",
    "                print(\"Pre-trained Model Loaded\")\n",
    "        \n",
    "        training_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            training_loss = 0\n",
    "            training_state = None\n",
    "            for step in range(steps_per_train_epoch):\n",
    "                offset = (step*batch_size) % (data['train'].shape[0]-batch_size)\n",
    "                batch_data = data['train'][offset:(offset+batch_size)]\n",
    "                batch_labels = data['labels'][offset:(offset+batch_size)]\n",
    "\n",
    "                feed_dict={g['x']: batch_data, g['y_sentiment']: batch_labels, g['embeddings']: data['embedding']}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, rnn_outputs_reshape, _ = sess.run([g['total_loss'],\n",
    "                                              g['final_state'],  \n",
    "                                              g['rnn_outputs_reshape'],            \n",
    "                                              g['train_step_unfreeze']],\n",
    "                                              feed_dict)\n",
    "                training_loss += training_loss_\n",
    "                if step==0:\n",
    "                    if epoch == 0:\n",
    "                        rnn_difference = np.array(rnn_outputs_reshape)\n",
    "                    else:\n",
    "                        rnn_mean_diff = np.mean(np.square(np.array(rnn_outputs_reshape) - rnn_difference))\n",
    "                        print(\"difference of rnn_ouputs: %f\" % rnn_mean_diff)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", epoch, \":\", training_loss/steps_per_train_epoch)\n",
    "            training_losses.append(training_loss/steps_per_train_epoch)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save, global_step=(epoch+1))\n",
    "            \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = EMBEDDING_DIM,\n",
    "    num_classes = nb_words,\n",
    "    num_labels = 3,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_steps = MAX_SEQUENCE_LENGTH,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    build_with_stopgradient = False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    with tf.variable_scope(\"global\"):\n",
    "        with tf.variable_scope(\"awd_lstm\"):\n",
    "            x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "            y_word = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "            y_sentiment = tf.placeholder(tf.int32, [batch_size], name='sentiments_placeholder')\n",
    "            embeddings = tf.placeholder(tf.float32, [num_classes, state_size], name='embeddings_placeholder')\n",
    "            dropout = tf.constant(1.0)\n",
    "\n",
    "            #embeddings = tf.get_variable('embedding_matrix', [nb_words, num_classes])\n",
    "            rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "            if cell_type == 'Custom':\n",
    "                cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "            elif cell_type == 'GRU':\n",
    "                cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "            elif cell_type == 'LSTM':\n",
    "                cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "            elif cell_type == 'LN_LSTM':\n",
    "                cell = LayerNormalizedLSTMCell(state_size)\n",
    "            else:\n",
    "                cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "            if build_with_dropout:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "            if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "            else:\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "            if build_with_dropout:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "            init_state = cell.zero_state(batch_size, tf.float32)\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "            rnn_outputs = tf.identity(rnn_outputs, 'rnn_outputs_tensor')\n",
    "            \n",
    "        #-------------------------------- NEXT WORDING PREDICTION --------------------------------------------    \n",
    "        with tf.variable_scope('word_prediction'):\n",
    "            W = tf.get_variable('W', [state_size, num_classes])\n",
    "            b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "            rnn_outputs_reshape = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "            word_logits = tf.matmul(rnn_outputs_reshape, W) + b\n",
    "            word_predictions = tf.nn.softmax(word_logits, name='predictions')\n",
    "            y_reshaped = tf.reshape(y_word, [-1])\n",
    "            word_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_reshaped, logits=word_logits, name=\"word_cross_entropy\")\n",
    "            word_cross_entropy_mean = tf.reduce_mean(word_cross_entropy, name=\"word_cross_entropy_mean\")\n",
    "            word_train_step = tf.train.AdamOptimizer(learning_rate).minimize(word_cross_entropy_mean)    \n",
    "\n",
    "        #------------------------------- SENTIMENT CLASSIFICATION --------------------------------------------\n",
    "        with tf.variable_scope('sentiment_softmax'):\n",
    "            W_s = tf.get_variable('W_s', [state_size, num_labels])\n",
    "            b_s = tf.get_variable('b_s', [num_labels], initializer=tf.constant_initializer(0.0))\n",
    "            rnn_outputs_mean = tf.reduce_mean(rnn_outputs, 1)\n",
    "            sentiment_logits = tf.matmul(rnn_outputs_mean, W_s) + b_s\n",
    "            sentiment_predictions = tf.nn.softmax(sentiment_logits, name='predictions_sentiment')\n",
    "            sentiment_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_sentiment, logits=sentiment_logits, name=\"cross_entropy_sentiment\")\n",
    "            sentiment_cross_entropy_mean = tf.reduce_mean(sentiment_cross_entropy, name=\"cross_entropy_sentiment_mean\")\n",
    "          \n",
    "        freeze_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"global/sentiment_softmax\")\n",
    "        sentiment_train_step_freeze = tf.train.AdamOptimizer(learning_rate).minimize(sentiment_cross_entropy_mean, var_list=freeze_train_vars)  \n",
    "        sentiment_train_step_unfreeze = tf.train.AdamOptimizer(learning_rate).minimize(sentiment_cross_entropy_mean)  \n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y_word = y_word,\n",
    "        y_sentiment = y_sentiment,\n",
    "        embeddings = embeddings,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = sentiment_cross_entropy_mean,\n",
    "        train_step_word = word_train_step,\n",
    "        train_step_freeze = sentiment_train_step_freeze,\n",
    "        train_step_unfreeze = sentiment_train_step_unfreeze,\n",
    "        rnn_outputs_reshape = rnn_outputs_reshape,\n",
    "        preds = sentiment_predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 1 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 2 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 3 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 4 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 5 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 6 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 7 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 8 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 9 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 10 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 11 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 12 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 13 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 14 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 15 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 16 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 17 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 18 : nan\n",
      "difference of rnn_ouputs: nan\n",
      "Average training loss for Epoch 19 : nan\n",
      "It took 2866.142467737198 seconds to train for 10 epochs.\n",
      "The average loss on the final epoch was: nan\n"
     ]
    }
   ],
   "source": [
    "g = build_graph(cell_type='LN_LSTM', num_steps=MAX_SEQUENCE_LENGTH)\n",
    "pretrain_model = \"base_model/LN_LSTM_word_prediction.ckpt\"\n",
    "save = \"benchmark_model/LN_LSTM_sentiment.ckpt\"\n",
    "t = time.time()\n",
    "losses = train_network(g, data, 20)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 10 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
