{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from NLPcleansing import prep\n",
    "#from customRNNgraph import RNN_build_graph\n",
    "from customRNNgraphBidrection import U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 18\n",
    "FORWARD_STEPS = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_epochs = 20\n",
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14640 tweets\n"
     ]
    }
   ],
   "source": [
    "# read data from twitter\n",
    "tweet_data   = pd.read_csv('datasets/us_airline.csv')\n",
    "columns = ['text', 'airline_sentiment']\n",
    "tweet_data_extract = tweet_data[:][columns]\n",
    "tweet_data_extract.dropna()\n",
    "tweet_data_extract[\"clean_text\"] = tweet_data_extract[\"text\"].map(lambda x: prep.text_to_wordlist(x))\n",
    "tweet_data_extract[\"labels\"] = tweet_data_extract[\"airline_sentiment\"].map(lambda x: prep.sentiment_to_label(x, ['neutral', 'positive', 'negative']))\n",
    "print('Found %s tweets' % len(tweet_data_extract[\"clean_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15074 tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenize tweets\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(tweet_data_extract[\"clean_text\"])\n",
    "sequence_tweets = tokenizer.texts_to_sequences(tweet_data_extract[\"clean_text\"])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('embedding_matrix.npy'):\n",
    "    embedding_matrix = np.load(\"embedding_matrix.npy\")\n",
    "else:\n",
    "    GloVe_file = \"datasets/glove.6B/glove.6B.300d.txt\"\n",
    "    word2vec_file = \"datasets/glove_word2vec.txt\"\n",
    "    embedding_matrix = prep.word2vec_GloVe(GloVe_file, word2vec_file, word_index=word_index)\n",
    "    np.save(\"embedding_matrix.npy\", embedding_matrix)\n",
    "nb_words = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sequence to sequence pairs\n",
    "sequence_tweets_filtered = []\n",
    "for i in range(len(sequence_tweets)):\n",
    "    if len(sequence_tweets[i]) >= (MAX_SEQUENCE_LENGTH + FORWARD_STEPS):\n",
    "        sequence_tweets_filtered.append(sequence_tweets[i])\n",
    "#data = pad_sequences(sequence_tweets_filtered, maxlen=(MAX_SEQUENCE_LENGTH + FORWARD_STEPS))\n",
    "#print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of train data: int32\n",
      "dtype of train label: int32\n",
      "shape of train data:  (40626, 18)\n",
      "shape of train label:  (40626, 18)\n",
      "TRIAN STEP 1270\n",
      "VALID STEP 142\n"
     ]
    }
   ],
   "source": [
    "train_seq = []\n",
    "label_seq = []\n",
    "for i in range(len(sequence_tweets_filtered)):\n",
    "    for j in range((len(sequence_tweets_filtered[i])-MAX_SEQUENCE_LENGTH-FORWARD_STEPS+1)):\n",
    "        train_seq.append(sequence_tweets_filtered[i][j:(j+MAX_SEQUENCE_LENGTH)])\n",
    "        label_seq.append(sequence_tweets_filtered[i][(j+FORWARD_STEPS):(j+MAX_SEQUENCE_LENGTH+FORWARD_STEPS)])\n",
    "np_train_seq = np.array(list(train_seq), dtype='int32')\n",
    "np_label_seq = np.array(list(label_seq), dtype='int32')\n",
    "\n",
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(np_train_seq))\n",
    "idx_train = perm[:int(len(np_train_seq)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(np_train_seq)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train = np_train_seq[idx_train]\n",
    "data_val = np_train_seq[idx_val]\n",
    "\n",
    "labels_train = np_label_seq[idx_train]\n",
    "labels_val = np_label_seq[idx_val]\n",
    "\n",
    "print('dtype of train data: %s' % data_train.dtype)\n",
    "print('dtype of train label: %s' % labels_train.dtype)\n",
    "\n",
    "print('shape of train data: ', data_train.shape)\n",
    "print('shape of train label: ', labels_train.shape)\n",
    "\n",
    "steps_per_train_epoch = int(data_train.shape[0]/BATCH_SIZE)+1\n",
    "steps_per_valid_epoch = int(data_val.shape[0]/BATCH_SIZE)+1\n",
    "print('TRIAN STEP %d' % steps_per_train_epoch)\n",
    "print('VALID STEP %d' % steps_per_valid_epoch)\n",
    "\n",
    "data = {'train_data': data_train, 'train_labels': labels_train, 'embedding': embedding_matrix, \n",
    "        'test_data': data_val, 'test_labels': labels_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/diyang/Downloads/ULMFiT-Tensorflow/customRNNgraphBidrection.py:72: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From /Users/diyang/Downloads/ULMFiT-Tensorflow/customRNNgraphBidrection.py:109: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "training start ...\n",
      "Average training loss for Epoch 0 : 5.501360336438878\n",
      "Average  testing loss for Epoch 0 : 3.672854577991324\n",
      "Average training loss for Epoch 1 : 2.7771549058711433\n",
      "Average  testing loss for Epoch 1 : 2.092655859362911\n",
      "Average training loss for Epoch 2 : 1.6654240530306899\n",
      "Average  testing loss for Epoch 2 : 1.3724791415980164\n",
      "Average training loss for Epoch 3 : 1.1080113868075092\n",
      "Average  testing loss for Epoch 3 : 0.98165878527601\n",
      "Average training loss for Epoch 4 : 0.7758367896549345\n",
      "Average  testing loss for Epoch 4 : 0.7431140840053558\n",
      "Average training loss for Epoch 5 : 0.5625262596945124\n",
      "Average  testing loss for Epoch 5 : 0.5914373540542495\n",
      "Average training loss for Epoch 6 : 0.4254212894073621\n",
      "Average  testing loss for Epoch 6 : 0.4991845643856156\n",
      "Average training loss for Epoch 7 : 0.3394332851011922\n",
      "Average  testing loss for Epoch 7 : 0.4476097221945373\n",
      "It took 12840.946438789368 seconds to train for 100 epochs.\n",
      "The average loss on the final epoch was: 0.3394332851011922\n"
     ]
    }
   ],
   "source": [
    "g_bidirect = RNN_bidirect_build_graph(state_size = EMBEDDING_DIM,\n",
    "                                      num_words = nb_words, \n",
    "                                      num_classes = 3, \n",
    "                                      batch_size = BATCH_SIZE,\n",
    "                                      sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "                                      cell_type = 'LN_LSTM',\n",
    "                                      num_layers = 3,\n",
    "                                      init_trainable = False)\n",
    "t = time.time()\n",
    "save = \"models/bidirect_lstm/base_bidirect/LN_LSTM_bidirect_word_prediction.ckpt\"\n",
    "losses = g_bidirect.train_base_model(data, 8, learning_rate = 1e-4, save=save, verbose_graph=False)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 100 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses['training_losses'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
